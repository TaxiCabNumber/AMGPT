# -*- coding: utf-8 -*-
"""llama2_rag.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-78epsa3IZr0_QoGbOo3wM8LQ8MKNQDp
"""

from google.colab import drive
drive.mount('/content/drive')

"""## RAG System Using Llama2 With Hugging Face"""

!pip install pypdf

!pip install -q transformers einops accelerate langchain bitsandbytes

## Embedding
!pip install install sentence_transformers

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-llms-huggingface
!pip install llama-index

from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import PromptTemplate

documents=SimpleDirectoryReader("/content/drive/MyDrive/Colab Notebooks/gpt_train_tex").load_data()
documents

SYSTEM_PROMPT = """You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:
- Generate human readable output, avoid creating output with gibberish text.
- Generate only the requested output, don't include any other language before or after the requested output.
- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.
- Generate professional language typically used in business documents in North America.
- Never generate offensive or foul language.
"""

query_wrapper_prompt = PromptTemplate(
    "[INST]<<SYS>>\n" + SYSTEM_PROMPT + "<</SYS>>\n\n{query_str}[/INST] "
)

!huggingface-cli login

import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.1, "do_sample": False},
    system_prompt=SYSTEM_PROMPT,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto",
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-embeddings-langchain

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index.core import ServiceContext
from llama_index.embeddings.langchain import LangchainEmbedding

embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

from llama_index.core import Settings

Settings.llm = llm
Settings.embed_model = embed_model

index=VectorStoreIndex.from_documents(documents)
index.storage_context.persist(persist_dir="/content/drive/MyDrive/Colab Notebooks/index")

index

query_engine=index.as_query_engine()

response=query_engine.query("what is energy density in laser powder bed fusion")

print(response)

response=query_engine.query("What is the relation between meltpool dimensions and density?")

print(response)

response=query_engine.query("How is phase composition of alloys analysed?")

query_str = "How is phase composition of alloys analysed?"
hyde = HyDEQueryTransform(include_original=True)
query_engine = index.as_query_engine()
query_engine = TransformQueryEngine(query_engine, query_transform=hyde)
response = query_engine.query(query_str)
print(response)

print(response)

response=query_engine.query("Explain slicing strategies in laser powder bed fusion" )

print(response)

response=query_engine.query("How do you obtain the temperature distribution in the heat-affected zone?" )

from IPython.display import display, Markdown, Latex, Math

print(response)

"""To obtain the temperature distribution within the heat-affected zone, we use the equation of heat transfer, which is given by:

$$\rho C_{\text{v}} \frac{\partial T}{\partial t} = K \left( \frac{\partial T}{\partial x} + \frac{\partial T}{\partial y} + \frac{\partial T}{\partial z} \right) + Q$$

where $\rho$ is the density, $C_{\text{v}}$ is the heat capacity, $K$ is the heat conductivity, $T$ is the temperature, and $Q$ is the internal heat generation per unit volume.

We solve this equation along each time step of the single-bead scan with the consideration of nonlinear temperature-dependent Ti-6Al-4V thermo-physical properties. The solidus temperature of the alloy Ti-6Al-4V corresponding to the melt pool boundary is taken as $1873 \text{ K}\left(1600^\circ \text{C}\right)$.

The temperature distribution is obtained by tracking the material state using 3DSIM FLEX simulation software, which solves the thermal diffusion
"""

response_str = str(response)

# Split the response into text and LaTeX parts
parts = []
current_part = ""
in_latex = False

for char in response_str:
    if char == "$":
        if in_latex:
            parts.append(Math(current_part))
            current_part = ""
        else:
            current_part += char
        in_latex = not in_latex
    else:
        current_part += char

if current_part:
    parts.append(current_part)

# Display the parts
for part in parts:
    if isinstance(part, str):
        print(part, end="")
    else:
        display(part)

response=query_engine.query("Why are popular in additive manufacturing?" )

print(response)